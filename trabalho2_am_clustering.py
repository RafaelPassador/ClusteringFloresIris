# -*- coding: utf-8 -*-
"""Trabalho2_AM_Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dcTuiJgoOdeLTEEYVnHViyCoZ1wpF-y-

---
<p align="left">
  <big>
    <b>
      <pre>
Camila Manara Ribeiro                - RA: 760465 
Júlia Aparecida Sousa de Oliveira    - RA: 769707 
Luciana Oliveira de Souza Gomes      - RA: 743569
Rafael Vinicius Polato Passador      - RA: 790036 
      </pre>
      <br>
      Disciplina: Aprendizado de Máquina
      <br>
      Professor: Prof. Dr. Diego Furtado Silva
      <br>
    </b>
  </big>
</p>


---

# Carregando Bibliotecas necessárias
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()  # for plot styling
import numpy as np
import pandas as pd
from pandas.plotting import andrews_curves

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn import preprocessing
from sklearn.cluster import KMeans

"""# Carregando Dataset"""

iris_dataset = pd.read_csv("Iris.csv")

iris_dataset

iris_dataset.head()

iris_dataset.columns

iris_dataset.isnull()

iris_dataset.describe()

""">**Gráfico de dispersão do comprimento e largura das sépalas**



"""

iris_dataset.plot(kind="scatter", x="SepalLengthCm",   y="SepalWidthCm")
plt.show()

"""> **Gráfico de dispersão do comprimento e largura das pétalas**"""

iris_dataset.plot(kind="scatter", x="PetalLengthCm",   y="PetalWidthCm")
plt.show()

"""> **Gráfico de Andrews das espécies**"""

andrews_curves(iris_dataset.drop("Id", axis=1), "Species")
plt.show()

"""# Modelando com K-means

O K-means é um algoritmo de agrupamento que visa particionar n observações em k agrupamentos.
1. **Inicialização** - K "meios" iniciais (centróides) são gerados aleatoriamente

2.  **Atribuição** - K clusters são criados associando cada observação com o centróide mais próximo.
3.  **Atualização** - O centróide dos clusters torna-se a nova média. 

Atribuição e Atualização são repetidos iterativamente até a convergência.
  
  O resultado é que a soma dos quadrados dos erros é minimizada entre os pontos e seus respectivos centróides. Usaremos o clustering do KMeans. Em primeiro lugar, encontraremos os clusters ideais com base na inércia e usando o método do cotovelo. A distância entre os centróides e os pontos de dados deve ser a menor.
"""

iris_dataset.drop('Id', axis=1, inplace=True)
iris_dataset.head()

"""Separamos os dados em duas variáveis:

*   **especies_target**: os dados alvos que contem as espécies
*   **clustering_data**: os dados de treinamento que contem os atributos utilizados para o clustering


"""

especies_target = iris_dataset.iloc[:,4]
especies_target.head()

clustering_data = iris_dataset.iloc[:,[0,1,2,3]]
clustering_data.head()

"""> **Representação gráfica das variáveis**"""

fig, ax = plt.subplots(figsize=(15,7))
sns.set(font_scale=1.5)
ax = sns.scatterplot(x=iris_dataset['SepalLengthCm'],y=iris_dataset['SepalWidthCm'], s=70, color='#0000ff', edgecolor='#0000ff', linewidth=0.3)
ax.set_ylabel('SepalWidthCm')
ax.set_xlabel('SepalLengthCm')
plt.title('Comprimento vs Largura das Sépalas', fontsize = 20)
plt.show()

"""## Modelo Pré-Elbow

>Como o K-means é um algoritimo não supervisionado, não utiliza os valores das classes para criar o modelo. Assim a variável X armazena apenas os valores dos atributos SepalLength, SepalWidth, PetalLength e PetalWidth.
"""

X = iris_dataset.iloc[:, 0:4].values

X

""">Inicialmente, vamos utilizar os valores padrão de paramentros da função Kmeans(), com o número de clusters = 8 e com o modo k-means++, em que os centroides são gerados de modo a favorecer a convergência.
>Além disso, o método fit() agrupa os dados e kmeans.cluster_centers mostra os centroides gerados, retornando um vetor com o número de instâncias igual ao número de clusters definido previamente.
"""

clustering = KMeans(n_clusters = 8, init = 'k-means++')
clustering.fit(X)
clustering.cluster_centers_

""">A função fit_transform() agrupa os dados retornando uma tabela de distâncias entre cada intância e o cluster gerado"""

tabela_distancia = clustering.fit_transform(X)
tabela_distancia

plt.scatter(X[:, 0], X[:,1], s = 100, c = clustering.labels_)
plt.scatter(clustering.cluster_centers_[:, 0], clustering.cluster_centers_[:, 1], s = 300, c = 'red',label = 'Centroids')
plt.title('Iris Clusters and Centroids')
plt.xlabel('SepalLength')
plt.ylabel('SepalWidth')
plt.legend()

plt.show()

"""##Método Elbow

Esse método realiza o agrupamento de k-means no dataset considerando um intervalo de valores para k, sendo que cada um desses calcula uma pontuação média para os clusters.

Usualmente, esse cálculo é feito pela soma dos quadrados das distâncias de cada ponto ao centro que lhe foi atribuído.
Assim, a partir da plotagem da métrica, faz-se a determinação do melhor de valor k pela visualização no gráfico. 

Ao observar a curva gerada tem-se um ponto de inflexão, o formato de “cotovelo” indicará o melhor k.
Esse método usa a soma de quadrados dentro do cluster (também chamado de wcss) em relação ao valor (número de clusters) para descobrir o valor ideal de grupos, a partir de teste da variância desses dados.  Esse valor de k é considerado ideal, quando em caso de aumento do número de clusters não haja um valor significativo de ganho. 

"""

wcss=[]
for i in range(1,11):
    kmeans = KMeans(i)
    kmeans.fit(clustering_data)
    wcss.append(kmeans.inertia_)
np.array(wcss)

fig, ax = plt.subplots(figsize=(14,8))
ax = plt.plot(range(1,11),wcss, linewidth=3, color="blue", marker ="8")
plt.axvline(x=3, ls='--')
plt.ylabel('Soma de quadrados dentro do cluster')
plt.xlabel('No. de Clusters (k)')
plt.title('Método Elbow', fontsize = 20)
plt.show()

"""##Modelo Pós-Elbow

Como determinamos o número otimizado de clusters para o nosso conjunto de dados, cria-se um novo modelo com n_clusters=3.

"""

clustering_elbow = KMeans(n_clusters=3, init='k-means++')
clustering_elbow.fit(clustering_data)

clusters = clustering_data.copy()
clusters['Cluster_Prediction'] = clustering_elbow.fit_predict(clustering_data)
clusters.head()

""">Aqui, vemos o novo vetor de centroides dos clusters gerados"""

clustering_elbow.cluster_centers_

fig, ax = plt.subplots(figsize=(15,7)) 
plt.scatter(x=clusters[clusters['Cluster_Prediction'] == 0]['SepalLengthCm'],
            y=clusters[clusters['Cluster_Prediction'] == 0]['SepalWidthCm'],
            s=70,edgecolor='teal', linewidth=0.3, c='teal', label='Iris-versicolor')


plt.scatter(x=clusters[clusters['Cluster_Prediction'] == 1]['SepalLengthCm'],
            y=clusters[clusters['Cluster_Prediction'] == 1]['SepalWidthCm'],
            s=70,edgecolor='lime', linewidth=0.3, c='lime', label='Iris-setosa')


plt.scatter(x=clusters[clusters['Cluster_Prediction'] == 2]['SepalLengthCm'],
            y=clusters[clusters['Cluster_Prediction'] == 2]['SepalWidthCm'],
            s=70,edgecolor='magenta', linewidth=0.3, c='magenta', label='Iris-virginica')

plt.scatter(x=clustering_elbow.cluster_centers_[:, 0], y=clustering_elbow.cluster_centers_[:, 1], s = 170, c = 'yellow', label = 'Centroids',edgecolor='black', linewidth=0.3)
plt.legend(loc='upper left')
plt.xlim(4,8)
plt.ylim(1.8,4.5)
ax.set_ylabel('Sepal Width (in cm)')
ax.set_xlabel('Sepal Length (in cm)')
plt.title('Clusters', fontsize = 20)
plt.show()

"""# DBSCAN

DBSCAN significa Clustering Espacial de Aplicatições com Ruído baseado em densidade. Pode-se perceber a partir de seu nome que divide o conjunto de dados em subgrupos com base em regiões densas. O DBSCAN não exige que forneçamos vários clusters antecipadamente.

Definimos 3 tipos diferentes de pontos para DBSCAN:

* Pontos centrais: Um ponto central é um ponto que tem pelo menos um número mínimo de outros pontos (MinPts) em seu épsilon de raio.

* Pontos de fronteira: Um ponto de fronteira é um ponto que não é um 
ponto central, uma vez que não tem MinPts suficiente em sua vizinhança, mas fica dentro do raio épsilon de um ponto central.

* Pontos de ruído: Todos os outros pontos que não são pontos centrais nem pontos de fronteira.
"""

atributos = iris_dataset.iloc[:,0:4]
especies = iris_dataset.iloc[:,4:]

le = preprocessing.LabelEncoder()
Y = le.fit_transform(especies)

Y

db = DBSCAN(eps=0.7,min_samples=2,algorithm='kd_tree').fit(atributos)

core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Número estimado de clusters: %d' % n_clusters_)
print('Número estimado de pontos de ruído: %d' % n_noise_)
print("Homogeneidade: %0.3f" % metrics.homogeneity_score(Y, labels))

unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]


for k,col in zip(unique_labels, colors):
  if k == -1:
    col=[0,0,0,1]
  
  class_member_mask = (labels == k)

  xy = atributos[class_member_mask & core_samples_mask]
  xy = xy.values
  plt.plot(xy[:,0:2], xy[:,2:4], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize = 14)

  xy = atributos[class_member_mask & ~core_samples_mask]
  xy = xy.values
  plt.plot(xy[:,2], xy[:,1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize = 14)
  

plt.title('Número estimado de clusters: %d' % n_clusters_)
plt.show()